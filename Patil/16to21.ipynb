{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30eb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\jupter\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\jupter\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\jupter\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in d:\\jupter\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in d:\\jupter\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in d:\\jupter\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7194bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Atharva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e00c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk- natural language toolkit\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d55e95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Using cached gradio-3.33.1-py3-none-any.whl (20.0 MB)\n",
      "Collecting websockets>=10.0\n",
      "  Using cached websockets-11.0.3-cp310-cp310-win_amd64.whl (124 kB)\n",
      "Requirement already satisfied: numpy in d:\\jupter\\lib\\site-packages (from gradio) (1.23.5)\n",
      "Collecting gradio-client>=0.2.4\n",
      "  Using cached gradio_client-0.2.5-py3-none-any.whl (288 kB)\n",
      "Collecting pygments>=2.12.0\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\jupter\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Collecting ffmpy\n",
      "  Using cached ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software while executing command python setup.py egg_info\n",
      "ERROR: Could not install packages due to an OSError: [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d86958e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#gradio is used for creating UI components\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "#gradio is used for creating UI components\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ba24d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a1231f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(input):\n",
    "    token= word_tokenize(input)\n",
    "    return token\n",
    "def sent_tokenization(input):\n",
    "    token= sent_tokenize(input)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ffd8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_tokenization= gr.Interface(word_tokenization, inputs=\"text\", outputs=\"text\")\n",
    "demo_tokenization.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e602ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo= gr.Blocks()\n",
    "with demo:\n",
    "    gr.Markdown(\"Tokenization\")\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Word Tokenization\"):\n",
    "            with gr.Row():\n",
    "                word_ip= gr.Textbox(label=\"Input Data\")\n",
    "                word_op= gr.Textbox(label=\"Output Data\")\n",
    "                word_btn= gr.Button(\"Generate Tokens\")\n",
    "        with gr.TabItem(\"Sentence Tokenization\"):\n",
    "            with gr.Row():\n",
    "                sent_ip= gr.Textbox(label=\"Input Data\")\n",
    "                sent_op= gr.Textbox(label=\"Output Data\")\n",
    "                sent_btn= gr.Button(\"Generate Tokens\")\n",
    "    word_btn.click(word_tokenization, inputs=word_ip, outputs=word_op)\n",
    "    sent_btn.click(sent_tokenization, inputs=sent_ip, outputs=sent_op)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f5b7c",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43187f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer= SnowballStemmer('english')\n",
    "def stemming(text):\n",
    "    tokenize_words= word_tokenize(text)\n",
    "    stemmed_list= [snowball_stemmer.stem(words) for words in tokenize_words]\n",
    "    return stemmed_list\n",
    "\n",
    "demo_stemming= gr.Interface(stemming, inputs=\"text\", outputs=\"text\")\n",
    "demo_stemming.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5fc2af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{',\n",
       " '\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2709',\n",
       " '\\\\cocoatextscaling0\\\\cocoaplatform0',\n",
       " '{',\n",
       " '\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0',\n",
       " 'helvetica',\n",
       " ';',\n",
       " '}',\n",
       " '{',\n",
       " '\\\\colortbl',\n",
       " ';',\n",
       " '\\\\red255\\\\green255\\\\blue255',\n",
       " ';',\n",
       " '}',\n",
       " '{',\n",
       " '\\\\',\n",
       " '*',\n",
       " '\\\\expandedcolortbl',\n",
       " ';',\n",
       " ';',\n",
       " '}',\n",
       " '\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0',\n",
       " '\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0',\n",
       " '\\\\f0\\\\fs24',\n",
       " '\\\\cf0',\n",
       " 'hi',\n",
       " ',',\n",
       " 'shivani',\n",
       " 'here',\n",
       " '.',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'are',\n",
       " 'do',\n",
       " 'well',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'convey',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'particip',\n",
       " 'in',\n",
       " 'the',\n",
       " 'contest',\n",
       " '.',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'you',\n",
       " '.',\n",
       " '}']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp1= open('stopwords.txt.rtf','r')\n",
    "input1= fp1.read()\n",
    "stemming(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d251845",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b383bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shivanidangal/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c3711bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/shivanidangal/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad4cf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer= WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    word_tokens= word_tokenize(text)\n",
    "    lemma= [lemmatizer.lemmatize(words) for words in word_tokens]\n",
    "    return lemma\n",
    "\n",
    "demo_lemma= gr.Interface(lemmatization, inputs=\"text\", outputs=\"text\")\n",
    "demo_lemma.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316c2d9",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03962500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shivanidangal/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63eb4074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words= stopwords.words('english')\n",
    "def stopwords_removal(text):\n",
    "    word_tokens= word_tokenize(text)\n",
    "    filtered_sent= [w for w in word_tokens if w.lower() not in stop_words]\n",
    "    filtered_sent=[]\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "    return filtered_sent\n",
    "\n",
    "demo_stopwords= gr.Interface(stopwords_removal, inputs=\"text\", outputs=\"text\")\n",
    "demo_stopwords.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2887efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2709', '\\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0', 'Helvetica;}', '{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}', '{\\\\*\\\\expandedcolortbl;;}', '\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0', '\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0', '\\\\f0\\\\fs24', '\\\\cf0', 'Hi,', 'Shivani', 'here.', 'Hope', 'well', 'I', 'wanted', 'convey', 'I', 'interested', 'participating', 'contest.', 'Looking', 'forward', 'work', 'you.}']\n"
     ]
    }
   ],
   "source": [
    "fp= open('stopwords.txt.rtf','r')\n",
    "file= fp.read()\n",
    "inp= file.split()\n",
    "def stopwords_file(text):\n",
    "    filtered= [w for w in text if w.lower() not in stop_words]\n",
    "    filtered= []\n",
    "    for w in text:\n",
    "        if w not in stop_words:\n",
    "            filtered.append(w)\n",
    "    return filtered\n",
    "result= stopwords_file(inp)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3563ed0",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f23319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shivanidangal/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4457df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos(text):\n",
    "    word_tokens= word_tokenize(text)\n",
    "    pos_list= nltk.pos_tag(word_tokens)\n",
    "    return pos_list\n",
    "\n",
    "demo_pos= gr.Interface(pos, inputs=\"text\", outputs=\"text\")\n",
    "demo_pos.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89258897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('{', '('), ('\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2709', 'VB'), ('\\\\cocoatextscaling0\\\\cocoaplatform0', 'NNP'), ('{', '('), ('\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0', 'NNP'), ('Helvetica', 'NNP'), (';', ':'), ('}', ')'), ('{', '('), ('\\\\colortbl', 'JJ'), (';', ':'), ('\\\\red255\\\\green255\\\\blue255', 'CC'), (';', ':'), ('}', ')'), ('{', '('), ('\\\\', 'JJ'), ('*', 'NNP'), ('\\\\expandedcolortbl', 'NNP'), (';', ':'), (';', ':'), ('}', ')'), ('\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0', 'FW'), ('\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0', 'JJ'), ('\\\\f0\\\\fs24', 'NNP'), ('\\\\cf0', 'NNP'), ('Hi', 'NNP'), (',', ','), ('Shivani', 'NNP'), ('here', 'RB'), ('.', '.'), ('Hope', 'NNP'), ('you', 'PRP'), ('are', 'VBP'), ('doing', 'VBG'), ('well', 'RB'), ('I', 'PRP'), ('just', 'RB'), ('wanted', 'VBD'), ('to', 'TO'), ('convey', 'VB'), ('that', 'IN'), ('I', 'PRP'), ('am', 'VBP'), ('interested', 'JJ'), ('in', 'IN'), ('participating', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('contest', 'NN'), ('.', '.'), ('Looking', 'VBG'), ('forward', 'RB'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('you', 'PRP'), ('.', '.'), ('}', ')')]\n"
     ]
    }
   ],
   "source": [
    "print(pos(input1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ee266",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3f0d2710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF values:\n",
      "  (0, 8)\t0.7071067811865476\n",
      "  (0, 4)\t0.7071067811865476\n",
      "  (1, 9)\t0.4403620672313486\n",
      "  (1, 6)\t0.4403620672313486\n",
      "  (1, 2)\t0.3349067026613031\n",
      "  (1, 0)\t0.4403620672313486\n",
      "  (1, 1)\t0.4403620672313486\n",
      "  (1, 8)\t0.3349067026613031\n",
      "  (2, 5)\t0.49047908420610337\n",
      "  (2, 3)\t0.49047908420610337\n",
      "  (2, 7)\t0.49047908420610337\n",
      "  (2, 2)\t0.3730219858594306\n",
      "  (2, 4)\t0.3730219858594306\n",
      "IDF values:\n",
      "daily : 1.6931471805599454\n",
      "do : 1.6931471805599454\n",
      "exercise : 1.2876820724517808\n",
      "for : 1.6931471805599454\n",
      "good : 1.2876820724517808\n",
      "health : 1.6931471805599454\n",
      "in : 1.6931471805599454\n",
      "is : 1.6931471805599454\n",
      "morning : 1.2876820724517808\n",
      "the : 1.6931471805599454\n",
      "Indexes: {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n"
     ]
    }
   ],
   "source": [
    "d0= \"good morning\"\n",
    "d1= \"do daily exercise in the morning\"\n",
    "d2= \"exercise is good for health\"\n",
    "series= [d0,d1,d2]\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "#tf-idf values\n",
    "res= tfidf.fit_transform(series)\n",
    "print('TF values:')\n",
    "print(res)\n",
    "\n",
    "#idf values\n",
    "print('IDF values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1,\":\",ele2)\n",
    "    \n",
    "print('Indexes:', tfidf.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c4082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
